{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f08b39e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -r requirements.txt --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b53467d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from os import environ\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from typing_extensions import TypedDict,Literal\n",
    "from langgraph.types import Command\n",
    "from langgraph.graph import MessagesState, StateGraph, START, END\n",
    "from IPython.display import Image, display\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from  user_tools import get_weather_tool\n",
    "from user_functions import vector_search  \n",
    "from utils import pretty_print_messages\n",
    "from langchain_core.messages import HumanMessage,SystemMessage\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "from azure.ai.projects import AIProjectClient\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from langchain_azure_ai.callbacks.tracers import AzureAIInferenceTracer\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "\n",
    "\n",
    "members = [\"document_search\", \"weather\"]\n",
    "\n",
    "options = members + [\"FINISH\"]\n",
    "\n",
    "system_prompt = (\n",
    "    \"You are a supervisor tasked with managing a conversation between the\"\n",
    "    f\" following workers: {members}. Given the following user request,\"\n",
    "    \" respond with the worker to act next. Each worker will perform a\"\n",
    "    \" task and respond with their results and status. When finished,\"\n",
    "    \" respond with FINISH.\"\n",
    ")\n",
    "\n",
    "class Router(TypedDict):\n",
    "    \"\"\"Worker to route to next. If no workers needed, route to FINISH.\"\"\"\n",
    "\n",
    "    next: Literal[*options]\n",
    "\n",
    "\n",
    "llm = AzureChatOpenAI(\n",
    "        temperature=0,\n",
    "        azure_deployment=environ[\"AZURE_OPENAI_MODEL\"],\n",
    "        api_version=environ[\"AZURE_OPENAI_API_VERSION\"]\n",
    "    )\n",
    "\n",
    "\n",
    "llm_with_tools = create_react_agent(llm, tools=[get_weather_tool])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f29764",
   "metadata": {},
   "source": [
    "## Configure tracing for Azure AI Foundry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab02a9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_client = AIProjectClient.from_connection_string(\n",
    "    credential=DefaultAzureCredential(),\n",
    "    conn_str=environ[\"AZURE_AI_PROJECT_CONNECTION_STRING\"],\n",
    ")\n",
    "\n",
    "application_insights_connection_string = project_client.telemetry.get_connection_string()\n",
    "\n",
    "tracer = AzureAIInferenceTracer(\n",
    "    connection_string=application_insights_connection_string,\n",
    "    enable_content_recording=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e6a186a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def document_search(state: MessagesState) -> Command[Literal[\"supervisor\"]]:\n",
    "    messages = state[\"messages\"]\n",
    "    last_user_message = next((msg for msg in reversed(messages)), None)\n",
    "    if not last_user_message:\n",
    "        return Command(goto=\"supervisor\")\n",
    "\n",
    "    query = last_user_message.content\n",
    "    context = vector_search(query)  \n",
    "\n",
    "    \n",
    "    messages.append(\n",
    "    SystemMessage(\n",
    "        content=(\n",
    "            f\"You are a helpful assistant. Use only the information in the context below to answer the user's question. \"\n",
    "            f\"If the context does not contain the answer, respond with \\\"I don't know.\\\"\\n\\nContext:\\n{context}\"\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "    response = llm.invoke(messages)\n",
    "\n",
    "    return Command(\n",
    "        update={\n",
    "            \"messages\": [\n",
    "                HumanMessage(content=response.content, name=\"document_search\")\n",
    "            ]\n",
    "        },\n",
    "        goto=\"supervisor\",\n",
    "    )\n",
    "\n",
    "\n",
    "def weather(state: MessagesState) -> Command[Literal[\"supervisor\"]]:\n",
    "    result = llm_with_tools.invoke(state)\n",
    "    return Command(\n",
    "        update={\n",
    "            \"messages\": [\n",
    "                HumanMessage(content=result[\"messages\"][-1].content, name=\"weather\")\n",
    "            ]\n",
    "        },\n",
    "        goto=\"supervisor\",\n",
    "    )\n",
    "\n",
    "def supervisor(state: MessagesState) -> Command[Literal[*members, \"__end__\"]]:\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "    ] + state[\"messages\"]\n",
    "    response = llm.with_structured_output(Router).invoke(messages)\n",
    "    goto = response[\"next\"]\n",
    "    if goto == \"FINISH\":\n",
    "        goto = END\n",
    "\n",
    "    return Command(goto=goto, update={\"next\": goto})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e79400e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "builder = StateGraph(MessagesState)\n",
    "builder.add_node(\"document_search\", document_search)\n",
    "builder.add_node(\"weather\", weather)\n",
    "builder.add_node(\"supervisor\", supervisor)\n",
    "\n",
    "builder.set_entry_point(\"supervisor\")\n",
    "\n",
    "graph = builder.compile()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0204c509",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Unable to display the graph visualization. This won't affect the rest of the notebook.\n",
      "Reason: Failed to reach https://mermaid.ink/ API while trying to render your graph after 1 retries. To resolve this issue:\n",
      "1. Check your internet connection and try again\n",
      "2. Try with higher retry settings: `draw_mermaid_png(..., max_retries=5, retry_delay=2.0)`\n",
      "3. Use the Pyppeteer rendering method which will render your graph locally in a browser: `draw_mermaid_png(..., draw_method=MermaidDrawMethod.PYPPETEER)`\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    display(Image(graph.get_graph().draw_mermaid_png()))\n",
    "except Exception as e:\n",
    "    print(\"⚠️ Unable to display the graph visualization. This won't affect the rest of the notebook.\")\n",
    "    print(f\"Reason: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce6d2dc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Update from node supervisor:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Update from node document_search:\n",
      "\n",
      "\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "Name: document_search\n",
      "\n",
      "The resource intensity of data center facility infrastructure is described using two key metrics: **Power Usage Effectiveness (PUE)** and **Water Usage Effectiveness (WUE)**. \n",
      "\n",
      "- **PUE** is the ratio of the total electricity demand of the data center to the electricity demand of the IT equipment. It is dimensionless (kWh/kWh) and reflects the efficiency of electricity use in the facility infrastructure.\n",
      "- **WUE** is the total water consumption of the data center divided by the electricity demand of the IT equipment, reported in liters per kWh. It primarily accounts for on-site water consumption associated with cooling systems.\n",
      "\n",
      "These metrics are influenced by factors such as cooling systems, operational practices, and climatic conditions. Additionally, U.S. data centers in 2023 had an indirect water footprint of nearly 800 billion liters due to electricity use, with an average water intensity of 4.52 liters per kWh.\n",
      "\n",
      "\n",
      "Update from node supervisor:\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for step in graph.stream({\"messages\": [{\"role\": \"user\", \"content\": \"Describe the resource intensity of data center facility infrastructure\"}]},config={\"callbacks\": [tracer]}):\n",
    "   pretty_print_messages(step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d6fa4e4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Update from node supervisor:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Update from node weather:\n",
      "\n",
      "\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "Name: weather\n",
      "\n",
      "The weather in San Francisco is currently 60 degrees and foggy.\n",
      "\n",
      "\n",
      "Update from node supervisor:\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for step in graph.stream({\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]},config={\"callbacks\": [tracer]}):\n",
    "   pretty_print_messages(step)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
