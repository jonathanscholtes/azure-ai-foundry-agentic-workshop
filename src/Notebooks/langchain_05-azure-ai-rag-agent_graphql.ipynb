{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f08b39e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -r requirements.txt --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b53467d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "from os import environ\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from typing_extensions import TypedDict,Literal\n",
    "from langgraph.types import Command\n",
    "from langgraph.graph import MessagesState, StateGraph, START, END\n",
    "from IPython.display import Image, display\n",
    "from  user_tools import get_weather_tool,convert_fahrenheit_to_celsius\n",
    "from user_functions import vector_search  \n",
    "from utils import pretty_print_messages\n",
    "from langchain_core.messages import HumanMessage,SystemMessage\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "from langchain_community.utilities.graphql import GraphQLAPIWrapper\n",
    "from langchain_community.tools.graphql.tool import BaseGraphQLTool\n",
    "\n",
    "import requests\n",
    "import json\n",
    "\n",
    "load_dotenv(override=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c46103a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "members = [\"document_search\", \"weather\", \"datacenter_energy_usage\"]\n",
    "\n",
    "options = members + [\"FINISH\"]\n",
    "\n",
    "system_prompt = (\n",
    "    \"You are a supervisor tasked with managing a conversation between the following workers: \"\n",
    "    f\"{members}. Given the following user request, respond with the worker to act next. Each worker will perform a task and \"\n",
    "    \"respond with their results and status. When finished, respond with 'FINISH'.\\n\\n\"\n",
    "    \"Note: The 'datacenter_energy_usage' worker handles queries related to data center tabular queries on energy, \"\n",
    "    \"power, or consumption topics. When returning datetime values, format them in the ISO 8601 standard (e.g., '2025-04-17T12:30:00Z').\\n\\n\"\n",
    "    \"Additionally, whenever a temperature in Fahrenheit (°F) is mentioned, \"\n",
    "    \"you must immediately and automatically convert it to Celsius (°C) using the available tool. \"\n",
    "    \"Do not ask the user for confirmation, clarification, or any additional input. \"\n",
    "    \"Always perform the conversion as part of the response.\\n\\n\"\n",
    ")\n",
    "\n",
    "\n",
    "class Router(TypedDict):\n",
    "    \"\"\"Worker to route to next. If no workers needed, route to FINISH.\"\"\"\n",
    "\n",
    "    next: Literal[*options]\n",
    "\n",
    "llm = AzureChatOpenAI(\n",
    "        temperature=0,\n",
    "        azure_deployment=environ[\"AZURE_OPENAI_MODEL\"],\n",
    "        api_version=environ[\"AZURE_OPENAI_API_VERSION\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e2730a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "graphql_wrapper = GraphQLAPIWrapper(graphql_endpoint=environ[\"GRAPHQL_URL\"])\n",
    "\n",
    "graphql_tool = BaseGraphQLTool(graphql_wrapper=graphql_wrapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "efbeb016",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [get_weather_tool, convert_fahrenheit_to_celsius]\n",
    "\n",
    "\n",
    "llm_with_tools = create_react_agent(llm, tools=tools)\n",
    "\n",
    "\n",
    "datacenter_agent = create_react_agent(\n",
    "    llm,\n",
    "    tools=[graphql_tool]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8cd6c6e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_graphql_schema_and_update_state(state: MessagesState):\n",
    "  \n",
    "    messages = state.get(\"messages\", [])\n",
    "    \n",
    "    # Define the introspection query\n",
    "    introspection_query = \"\"\"\n",
    "    {\n",
    "      __schema {\n",
    "        types {\n",
    "          name\n",
    "          fields {\n",
    "            name\n",
    "          }\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    introspection_response = json.loads(graphql_wrapper.run(introspection_query))\n",
    "    schema_data = introspection_response.get(\"__schema\", {}).get(\"types\", [])\n",
    "    schema_content = json.dumps(schema_data, indent=2)\n",
    "    \n",
    "    messages.append(\n",
    "        SystemMessage(content=f\"GraphQL Schema:\\n{schema_content}\")\n",
    "    )\n",
    "    \n",
    "    state[\"messages\"] = messages\n",
    "    state[\"graphql_schema\"] = schema_data  \n",
    "    \n",
    "    return state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4e46f16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def document_search(state: MessagesState) -> Command[Literal[\"supervisor\"]]:\n",
    "    messages = state[\"messages\"]\n",
    "    last_user_message = next((msg for msg in reversed(messages)), None)\n",
    "    if not last_user_message:\n",
    "        return Command(goto=\"supervisor\")\n",
    "\n",
    "    query = last_user_message.content\n",
    "    context = vector_search(query)  \n",
    "\n",
    "    \n",
    "    messages.append(\n",
    "    SystemMessage(\n",
    "        content=(\n",
    "            f\"You are a helpful assistant. Use only the information in the context below to answer the user's question. \"\n",
    "            f\"If the context does not contain the answer, respond with \\\"I don't know.\\\"\\n\\nContext:\\n{context}\"\n",
    "        )\n",
    "    )\n",
    "    )\n",
    "\n",
    "    response = llm.invoke(messages)\n",
    "\n",
    "    return Command(\n",
    "        update={\n",
    "            \"messages\": [\n",
    "                HumanMessage(content=response.content, name=\"document_search\")\n",
    "            ]\n",
    "        },\n",
    "        goto=\"supervisor\",\n",
    "    )\n",
    "\n",
    "\n",
    "def weather(state: MessagesState) -> Command[Literal[\"supervisor\"]]:\n",
    "    \n",
    "    result = llm_with_tools.invoke(state)\n",
    "\n",
    "    return Command(\n",
    "        update={\n",
    "            \"messages\": [\n",
    "                HumanMessage(content=result[\"messages\"][-1].content, name=\"weather\")\n",
    "            ]\n",
    "        },\n",
    "        goto=\"supervisor\",\n",
    "    )\n",
    "\n",
    "\n",
    "def datacenter_energy_usage(state: MessagesState) -> Command[Literal[\"supervisor\"]]:\n",
    "    state = get_graphql_schema_and_update_state(state)\n",
    "    result = datacenter_agent.invoke(state)\n",
    "    return Command(\n",
    "        update={\n",
    "            \"messages\": [\n",
    "                HumanMessage(content=result[\"messages\"][-1].content, name=\"datacenter_energy_usage\")\n",
    "            ]\n",
    "        },\n",
    "        goto=\"supervisor\",\n",
    "    )\n",
    "\n",
    "\n",
    "def supervisor(state: MessagesState) -> Command[Literal[*members, \"__end__\"]]:\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt}, \n",
    "    ] + state[\"messages\"]\n",
    "\n",
    "    \n",
    "    \n",
    "    response = llm.with_structured_output(Router).invoke(messages)\n",
    "\n",
    "\n",
    "    \n",
    "    goto = response[\"next\"]\n",
    "    if goto == \"FINISH\":\n",
    "        goto = END\n",
    "\n",
    "    return Command(goto=goto, update={\"next\": goto})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e79400e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "builder = StateGraph(MessagesState)\n",
    "builder.add_node(\"document_search\", document_search)\n",
    "builder.add_node(\"weather\", weather)\n",
    "builder.add_node(\"datacenter_energy_usage\", datacenter_energy_usage)\n",
    "builder.add_node(\"supervisor\", supervisor)\n",
    "\n",
    "builder.set_entry_point(\"supervisor\")\n",
    "\n",
    "graph = builder.compile()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0204c509",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Unable to display the graph visualization. This won't affect the rest of the notebook.\n",
      "Reason: Failed to reach https://mermaid.ink/ API while trying to render your graph after 1 retries. To resolve this issue:\n",
      "1. Check your internet connection and try again\n",
      "2. Try with higher retry settings: `draw_mermaid_png(..., max_retries=5, retry_delay=2.0)`\n",
      "3. Use the Pyppeteer rendering method which will render your graph locally in a browser: `draw_mermaid_png(..., draw_method=MermaidDrawMethod.PYPPETEER)`\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    display(Image(graph.get_graph().draw_mermaid_png()))\n",
    "except Exception as e:\n",
    "    print(\"⚠️ Unable to display the graph visualization. This won't affect the rest of the notebook.\")\n",
    "    print(f\"Reason: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ce6d2dc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Update from node supervisor:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Update from node datacenter_energy_usage:\n",
      "\n",
      "\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "Name: datacenter_energy_usage\n",
      "\n",
      "Here are the recorded temperatures for various data centers and zones:\n",
      "\n",
      "1. **Data Center ID:** DC-CHI2  \n",
      "   **Zone:** C3  \n",
      "   **Temperature:** 22.49°C  \n",
      "   **Timestamp:** 2025-04-14T15:15:13.724337  \n",
      "\n",
      "2. **Data Center ID:** DC-NYC1  \n",
      "   **Zone:** B2  \n",
      "   **Temperature:** 29.66°C  \n",
      "   **Timestamp:** 2025-04-14T15:00:13.724337  \n",
      "\n",
      "3. **Data Center ID:** DC-NYC1  \n",
      "   **Zone:** B2  \n",
      "   **Temperature:** 23.13°C  \n",
      "   **Timestamp:** 2025-04-14T14:45:13.724337  \n",
      "\n",
      "4. **Data Center ID:** DC-CHI2  \n",
      "   **Zone:** C3  \n",
      "   **Temperature:** 21.43°C  \n",
      "   **Timestamp:** 2025-04-14T14:30:13.724337  \n",
      "\n",
      "5. **Data Center ID:** DC-CHI2  \n",
      "   **Zone:** B2  \n",
      "   **Temperature:** 24.26°C  \n",
      "   **Timestamp:** 2025-04-14T14:15:13.724337  \n",
      "\n",
      "6. **Data Center ID:** DC-CHI2  \n",
      "   **Zone:** A1  \n",
      "   **Temperature:** 24.69°C  \n",
      "   **Timestamp:** 2025-04-14T14:00:13.724337  \n",
      "\n",
      "...and many more.\n",
      "\n",
      "Let me know if you'd like to see additional records or filter by specific data centers/zones!\n",
      "\n",
      "\n",
      "Update from node supervisor:\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for step in graph.stream({\"messages\": [{\"role\": \"user\", \"content\": \"Return Data Center Temperatures?\"}]}):\n",
    "   pretty_print_messages(step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d6fa4e4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Update from node supervisor:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Update from node document_search:\n",
      "\n",
      "\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "Name: document_search\n",
      "\n",
      "The data center space types considered in the study are:\n",
      "\n",
      "1. **Telco Edge**: Small closets/rooms to micro data centers and network infrastructure deployed by communications companies as points of presence throughout their network.\n",
      "\n",
      "2. **Commercial Edge**: Network closets, server rooms, and micro-data centers supporting digital infrastructure and software delivery services for commercial and industrial operations.\n",
      "\n",
      "3. **Small and Medium Businesses (SMB)**: Data center deployments in internal facilities of small and medium businesses.\n",
      "\n",
      "4. **Enterprise Branch**: Remote and branch office (ROBO) deployments for large enterprises in their own facilities, such as network closets and server rooms.\n",
      "\n",
      "5. **Internal**: Data centers run internally by enterprises for their own use.\n",
      "\n",
      "6. **Communications Service Providers (Comms SPs)**: Data centers operated by telecommunications or cable companies to support internal services for providing communications technology services.\n",
      "\n",
      "7. **Colocation – Small/Medium Scale**: Data centers built by local colocation companies, typically offering retail leasing at a smaller scale.\n",
      "\n",
      "8. **Colocation – Large Scale**: Data centers built by major colocation companies, providing wholesale and retail colocation leasing, often deploying large and mega data centers.\n",
      "\n",
      "9. **Hyperscale**: Data centers built by companies deploying internet services and platforms at massive scale.\n",
      "\n",
      "\n",
      "Update from node supervisor:\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for step in graph.stream({\"messages\": [{\"role\": \"user\", \"content\": \"What are the Data Center Space Types?\"}]}):\n",
    "   pretty_print_messages(step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1e89ed35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Update from node supervisor:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Update from node weather:\n",
      "\n",
      "\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "Name: weather\n",
      "\n",
      "The weather in San Francisco is currently 60°F and foggy.\n",
      "\n",
      "\n",
      "Update from node supervisor:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Update from node weather:\n",
      "\n",
      "\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "Name: weather\n",
      "\n",
      "The weather in San Francisco is currently 60°F and foggy.\n",
      "\n",
      "\n",
      "Update from node supervisor:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Update from node weather:\n",
      "\n",
      "\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "Name: weather\n",
      "\n",
      "The current weather in San Francisco is 60°F and foggy. Would you like me to convert the temperature to Celsius?\n",
      "\n",
      "\n",
      "Update from node supervisor:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Update from node weather:\n",
      "\n",
      "\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "Name: weather\n",
      "\n",
      "The current weather in San Francisco is 15.6°C and foggy.\n",
      "\n",
      "\n",
      "Update from node supervisor:\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for step in graph.stream({\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]}):\n",
    "   pretty_print_messages(step)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
