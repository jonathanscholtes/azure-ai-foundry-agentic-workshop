{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92a4c472",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -r requirements.txt --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad0649f",
   "metadata": {},
   "source": [
    "# Building Agents with Semantic Kernel Plugins\n",
    "\n",
    "This notebook demonstrates how to use the **Semantic Kernel Agent Framework** with **Plugins** to enable agents to perform complex tasks through structured function calling.\n",
    "\n",
    " ðŸ”— [Configuring Agents with Semantic Kernel Plugins](https://learn.microsoft.com/en-us/semantic-kernel/frameworks/agent/agent-functions?pivots=programming-language-python)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6c3f6ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantic_kernel import Kernel\n",
    "from user_plugins import WeatherPlugin\n",
    "from semantic_kernel.agents import ChatCompletionAgent\n",
    "from semantic_kernel.connectors.ai.open_ai import AzureChatCompletion\n",
    "from dotenv import load_dotenv\n",
    "from os import environ\n",
    "\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "kernel = Kernel()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5e822405",
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel.add_service(AzureChatCompletion(\n",
    "    service_id=\"chat\",\n",
    "    deployment_name=environ[\"AZURE_OPENAI_MODEL\"],\n",
    "    endpoint=environ[\"AZURE_OPENAI_ENDPOINT\"],\n",
    "    api_key=environ[\"AZURE_OPENAI_API_KEY\"] ))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "70d27331",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KernelPlugin(name='Weather', description=None, functions={'get_sunrise_sunset': KernelFunctionFromMethod(metadata=KernelFunctionMetadata(name='get_sunrise_sunset', plugin_name='Weather', description='Call to get the sunrise and sunset for a given location.', parameters=[KernelParameterMetadata(name='location', description='The location to get the sunrise and sunset', default_value=None, type_='str', is_required=True, type_object=<class 'str'>, schema_data={'type': 'string', 'description': 'The location to get the sunrise and sunset'}, include_in_function_choices=True)], is_prompt=False, is_asynchronous=True, return_parameter=KernelParameterMetadata(name='return', description='', default_value=None, type_='Any', is_required=True, type_object=None, schema_data={'type': 'object'}, include_in_function_choices=True), additional_properties={}), invocation_duration_histogram=<opentelemetry.metrics._internal.instrument._ProxyHistogram object at 0x00000279A2437440>, streaming_duration_histogram=<opentelemetry.metrics._internal.instrument._ProxyHistogram object at 0x00000279A23B3B90>, method=<function WeatherPlugin.get_sunrise_sunset at 0x000002799F323F60>, stream_method=None), 'get_weather': KernelFunctionFromMethod(metadata=KernelFunctionMetadata(name='get_weather', plugin_name='Weather', description='Call to get the current weather.', parameters=[KernelParameterMetadata(name='location', description='The location to get the current weather', default_value=None, type_='str', is_required=True, type_object=<class 'str'>, schema_data={'type': 'string', 'description': 'The location to get the current weather'}, include_in_function_choices=True)], is_prompt=False, is_asynchronous=True, return_parameter=KernelParameterMetadata(name='return', description='', default_value=None, type_='Any', is_required=True, type_object=None, schema_data={'type': 'object'}, include_in_function_choices=True), additional_properties={}), invocation_duration_histogram=<opentelemetry.metrics._internal.instrument._ProxyHistogram object at 0x00000279A24C4E60>, streaming_duration_histogram=<opentelemetry.metrics._internal.instrument._ProxyHistogram object at 0x00000279A24C5550>, method=<function WeatherPlugin.get_weather at 0x000002799F32C720>, stream_method=None)})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kernel.add_plugin(WeatherPlugin, plugin_name=\"Weather\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "95e66d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the agent\n",
    "agent = ChatCompletionAgent(\n",
    "    kernel=kernel, \n",
    "    name=\"WeatherAgent\", \n",
    "    instructions=(\n",
    "        \"You are an intelligent weather assistant. \"\n",
    "        \"Use the available plugins and tools to answer questions about the weather, \"\n",
    "        \"including forecasts, temperatures, and conditions. \"\n",
    "        \"Always include temperatures in both Celsius and Fahrenheit. \"\n",
    "        \"Be concise, friendly, and helpful. If you're unsure about a location, ask for clarification.\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2babf387",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The weather in San Francisco today is sunny with a temperature of 90Â°F (32Â°C). Stay cool and hydrated!\n"
     ]
    }
   ],
   "source": [
    "response = await agent.get_response(messages=\"What's the weather like in sf today?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d503630c",
   "metadata": {},
   "source": [
    "## [Handling Intermediate Messages with a ChatCompletionAgent](https://learn.microsoft.com/en-us/semantic-kernel/frameworks/agent/chat-completion-agent?pivots=programming-language-python#handling-intermediate-messages-with-a-chatcompletionagent)\n",
    "\n",
    "The Semantic Kernel ChatCompletionAgent is designed to invoke an agent that fulfills user queries or questions. During invocation, the agent may execute tools to derive the final answer. To access intermediate messages produced during this process, callers can supply a callback function that handles instances of FunctionCallContent or FunctionResultContent.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f39566ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# AuthorRole.USER: 'Hello'\n",
      "# WeatherAgent: Hi there! How can I assist you with the weather today? ðŸ˜Š\n",
      "# AuthorRole.USER: 'What's the weather like in sf today?'\n",
      "# WeatherAgent: The weather in San Francisco today is foggy with a temperature of 60Â°F (15.6Â°C). Let me know if you'd like further details! ðŸŒ«ï¸\n",
      "# AuthorRole.USER: 'What time is the sunrise?'\n",
      "# WeatherAgent: In San Francisco, the sunrise is at 6:05 AM today. Let me know if you need anything else! â˜€ï¸\n",
      "# AuthorRole.USER: 'Thank you'\n",
      "# WeatherAgent: You're very welcome! Have a fantastic day! ðŸŒŸ\n",
      "\n",
      "Intermediate Steps:\n",
      "Function Call:> Weather-get_weather with arguments: {\"location\":\"sf\"}\n",
      "Function Result:> It's 60 degrees and foggy. for function: Weather-get_weather\n",
      "Function Call:> Weather-get_sunrise_sunset with arguments: {\"location\":\"sf\"}\n",
      "Function Result:> Sunrise: 6:05 A.M, Sunset: 8:15 P.M for function: Weather-get_sunrise_sunset\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from semantic_kernel.contents import AuthorRole, ChatMessageContent, FunctionCallContent, FunctionResultContent\n",
    "\n",
    "\n",
    "# Define a list to hold callback message content\n",
    "intermediate_steps: list[ChatMessageContent] = []\n",
    "\n",
    "# Define an async method to handle the `on_intermediate_message` callback\n",
    "async def handle_intermediate_steps(message: ChatMessageContent) -> None:\n",
    "    intermediate_steps.append(message)\n",
    "\n",
    "\n",
    "\n",
    "user_inputs = [\n",
    "    \"Hello\", \n",
    "    \"What's the weather like in sf today?\", \n",
    "    \"What time is the sunrise?\",\n",
    "    \"Thank you\",\n",
    "]\n",
    "\n",
    "thread = None\n",
    "\n",
    "# Generate the agent response(s)\n",
    "for user_input in user_inputs:\n",
    "    print(f\"# {AuthorRole.USER}: '{user_input}'\")\n",
    "    async for response in agent.invoke(\n",
    "        messages=user_input,\n",
    "        thread=thread,\n",
    "        on_intermediate_message=handle_intermediate_steps,\n",
    "    ):\n",
    "        thread = response.thread\n",
    "        print(f\"# {response.name}: {response.content}\")\n",
    "\n",
    "# Delete the thread when it is no longer needed\n",
    "await thread.delete() if thread else None\n",
    "\n",
    "# Print the intermediate steps\n",
    "print(\"\\nIntermediate Steps:\")\n",
    "for msg in intermediate_steps:\n",
    "    if any(isinstance(item, FunctionResultContent) for item in msg.items):\n",
    "        for fr in msg.items:\n",
    "            if isinstance(fr, FunctionResultContent):\n",
    "                print(f\"Function Result:> {fr.result} for function: {fr.name}\")\n",
    "    elif any(isinstance(item, FunctionCallContent) for item in msg.items):\n",
    "        for fcc in msg.items:\n",
    "            if isinstance(fcc, FunctionCallContent):\n",
    "                print(f\"Function Call:> {fcc.name} with arguments: {fcc.arguments}\")\n",
    "    else:\n",
    "        print(f\"{msg.role}: {msg.content}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
