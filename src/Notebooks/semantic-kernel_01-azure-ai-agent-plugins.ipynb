{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92a4c472",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -r requirements.txt --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3f6ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantic_kernel import Kernel\n",
    "from user_plugins import WeatherPlugin\n",
    "from semantic_kernel.agents import ChatCompletionAgent\n",
    "from dotenv import load_dotenv\n",
    "from os import environ\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "kernel = Kernel()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "912f22b2",
   "metadata": {},
   "source": [
    "## üîç Tracing with Azure AI Foundry\n",
    "\n",
    "To enable tracing, use the `AzureAIInferenceChatCompletion` connector from Semantic Kernel. This connector automatically logs telemetry data that can be visualized in the Azure AI Foundry Tracing UI.\n",
    "\n",
    "üîó [View traces in Azure AI Foundry](https://learn.microsoft.com/en-us/semantic-kernel/concepts/enterprise-readiness/observability/telemetry-with-azure-ai-foundry-tracing#use-the-azure-ai-inference-connector)\n",
    "\n",
    "> ‚ö†Ô∏è If traces do not appear in the UI, you can implement a custom wrapper around the `AzureChatCompletion` connector to enable telemetry manually.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5e822405",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Use the Azure AI Inference connector\n",
    "\n",
    "from semantic_kernel.connectors.ai.azure_ai_inference import AzureAIInferenceChatCompletion\n",
    "from azure.ai.inference.aio import ChatCompletionsClient\n",
    "\n",
    "deployment_name=environ[\"AZURE_OPENAI_MODEL\"]\n",
    "endpoint=environ[\"AZURE_OPENAI_ENDPOINT\"]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f5f559fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel.add_service(AzureAIInferenceChatCompletion(\n",
    "    ai_model_id=deployment_name,\n",
    "    client=ChatCompletionsClient(\n",
    "        endpoint=f\"{str(endpoint).strip('/')}/openai/deployments/{deployment_name}\",\n",
    "        credential=AzureKeyCredential(environ[\"AZURE_OPENAI_API_KEY\"]),\n",
    "        credential_scopes=[\"https://cognitiveservices.azure.com/.default\"],\n",
    "    ),\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c71d6c",
   "metadata": {},
   "source": [
    "> ‚è≥ It may take a few minutes for the traces to show up on the UI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "70d27331",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KernelPlugin(name='Weather', description=None, functions={'get_sunrise_sunset': KernelFunctionFromMethod(metadata=KernelFunctionMetadata(name='get_sunrise_sunset', plugin_name='Weather', description='Call to get the sunrise and sunset for a given location.', parameters=[KernelParameterMetadata(name='location', description='The location to get the sunrise and sunset', default_value=None, type_='str', is_required=True, type_object=<class 'str'>, schema_data={'type': 'string', 'description': 'The location to get the sunrise and sunset'}, include_in_function_choices=True)], is_prompt=False, is_asynchronous=True, return_parameter=KernelParameterMetadata(name='return', description='', default_value=None, type_='Any', is_required=True, type_object=None, schema_data={'type': 'object'}, include_in_function_choices=True), additional_properties={}), invocation_duration_histogram=<opentelemetry.metrics._internal.instrument._ProxyHistogram object at 0x00000279A2432120>, streaming_duration_histogram=<opentelemetry.metrics._internal.instrument._ProxyHistogram object at 0x00000279A2431D90>, method=<function WeatherPlugin.get_sunrise_sunset at 0x000002799F323F60>, stream_method=None), 'get_weather': KernelFunctionFromMethod(metadata=KernelFunctionMetadata(name='get_weather', plugin_name='Weather', description='Call to get the current weather.', parameters=[KernelParameterMetadata(name='location', description='The location to get the current weather', default_value=None, type_='str', is_required=True, type_object=<class 'str'>, schema_data={'type': 'string', 'description': 'The location to get the current weather'}, include_in_function_choices=True)], is_prompt=False, is_asynchronous=True, return_parameter=KernelParameterMetadata(name='return', description='', default_value=None, type_='Any', is_required=True, type_object=None, schema_data={'type': 'object'}, include_in_function_choices=True), additional_properties={}), invocation_duration_histogram=<opentelemetry.metrics._internal.instrument._ProxyHistogram object at 0x00000279A2433320>, streaming_duration_histogram=<opentelemetry.metrics._internal.instrument._ProxyHistogram object at 0x00000279A2431340>, method=<function WeatherPlugin.get_weather at 0x000002799F32C720>, stream_method=None)})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kernel.add_plugin(WeatherPlugin, plugin_name=\"Weather\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "95e66d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the agent\n",
    "agent = ChatCompletionAgent(\n",
    "    kernel=kernel, \n",
    "    name=\"WeatherAgent\", \n",
    "    instructions=(\n",
    "        \"You are an intelligent weather assistant. \"\n",
    "        \"Use the available plugins and tools to answer questions about the weather, \"\n",
    "        \"including forecasts, temperatures, and conditions. \"\n",
    "        \"Always include temperatures in both Celsius and Fahrenheit. \"\n",
    "        \"Be concise, friendly, and helpful. If you're unsure about a location, ask for clarification.\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2babf387",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "San Francisco is sunny today with a temperature of 90¬∞F (32¬∞C). Enjoy the warm weather!\n"
     ]
    }
   ],
   "source": [
    "response = await agent.get_response(messages=\"What's the weather like in sf today?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d503630c",
   "metadata": {},
   "source": [
    "## [Handling Intermediate Messages with a ChatCompletionAgent](https://learn.microsoft.com/en-us/semantic-kernel/frameworks/agent/chat-completion-agent?pivots=programming-language-python#handling-intermediate-messages-with-a-chatcompletionagent)\n",
    "\n",
    "The Semantic Kernel ChatCompletionAgent is designed to invoke an agent that fulfills user queries or questions. During invocation, the agent may execute tools to derive the final answer. To access intermediate messages produced during this process, callers can supply a callback function that handles instances of FunctionCallContent or FunctionResultContent.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f39566ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# AuthorRole.USER: 'Hello'\n",
      "# WeatherAgent: Hi there! How can I assist you with the weather today? üòä\n",
      "# AuthorRole.USER: 'What's the weather like in sf today?'\n",
      "# WeatherAgent: The current weather in San Francisco is foggy, with a temperature of 60¬∞F (15¬∞C). Let me know if you need more details! üåÅ\n",
      "# AuthorRole.USER: 'What time is the sunrise?'\n",
      "# WeatherAgent: The sunrise in San Francisco today was at 6:05 AM, and the sunset will be at 8:15 PM. Let me know if you'd like more updates! üåÖ\n",
      "# AuthorRole.USER: 'Thank you'\n",
      "# WeatherAgent: You're very welcome! Have a fantastic day! üòä\n",
      "\n",
      "Intermediate Steps:\n",
      "Function Call:> Weather-get_weather with arguments: {\"location\":\"San Francisco\"}\n",
      "Function Result:> It's 60 degrees and foggy. for function: Weather-get_weather\n",
      "Function Call:> Weather-get_sunrise_sunset with arguments: {\"location\":\"San Francisco\"}\n",
      "Function Result:> Sunrise: 6:05 A.M, Sunset: 8:15 P.M for function: Weather-get_sunrise_sunset\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from semantic_kernel.contents import AuthorRole, ChatMessageContent, FunctionCallContent, FunctionResultContent\n",
    "\n",
    "\n",
    "# Define a list to hold callback message content\n",
    "intermediate_steps: list[ChatMessageContent] = []\n",
    "\n",
    "# Define an async method to handle the `on_intermediate_message` callback\n",
    "async def handle_intermediate_steps(message: ChatMessageContent) -> None:\n",
    "    intermediate_steps.append(message)\n",
    "\n",
    "\n",
    "\n",
    "user_inputs = [\n",
    "    \"Hello\", \n",
    "    \"What's the weather like in sf today?\", \n",
    "    \"What time is the sunrise?\",\n",
    "    \"Thank you\",\n",
    "]\n",
    "\n",
    "thread = None\n",
    "\n",
    "# Generate the agent response(s)\n",
    "for user_input in user_inputs:\n",
    "    print(f\"# {AuthorRole.USER}: '{user_input}'\")\n",
    "    async for response in agent.invoke(\n",
    "        messages=user_input,\n",
    "        thread=thread,\n",
    "        on_intermediate_message=handle_intermediate_steps,\n",
    "    ):\n",
    "        thread = response.thread\n",
    "        print(f\"# {response.name}: {response.content}\")\n",
    "\n",
    "# Delete the thread when it is no longer needed\n",
    "await thread.delete() if thread else None\n",
    "\n",
    "# Print the intermediate steps\n",
    "print(\"\\nIntermediate Steps:\")\n",
    "for msg in intermediate_steps:\n",
    "    if any(isinstance(item, FunctionResultContent) for item in msg.items):\n",
    "        for fr in msg.items:\n",
    "            if isinstance(fr, FunctionResultContent):\n",
    "                print(f\"Function Result:> {fr.result} for function: {fr.name}\")\n",
    "    elif any(isinstance(item, FunctionCallContent) for item in msg.items):\n",
    "        for fcc in msg.items:\n",
    "            if isinstance(fcc, FunctionCallContent):\n",
    "                print(f\"Function Call:> {fcc.name} with arguments: {fcc.arguments}\")\n",
    "    else:\n",
    "        print(f\"{msg.role}: {msg.content}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
