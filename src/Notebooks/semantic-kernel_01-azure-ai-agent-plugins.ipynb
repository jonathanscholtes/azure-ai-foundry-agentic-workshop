{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "92a4c472",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "! pip install -r requirements.txt --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad0649f",
   "metadata": {},
   "source": [
    "# Building Agents with Semantic Kernel Plugins\n",
    "\n",
    "This notebook demonstrates how to use the **Semantic Kernel Agent Framework** with **Plugins** to enable agents to perform complex tasks through structured function calling.\n",
    "\n",
    " ðŸ”— [Configuring Agents with Semantic Kernel Plugins](https://learn.microsoft.com/en-us/semantic-kernel/frameworks/agent/agent-functions?pivots=programming-language-python)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c3f6ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantic_kernel import Kernel\n",
    "from user_plugins import WeatherPlugin\n",
    "from semantic_kernel.agents import ChatCompletionAgent\n",
    "from semantic_kernel.connectors.ai.open_ai import AzureChatCompletion\n",
    "from dotenv import load_dotenv\n",
    "from os import environ\n",
    "\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "kernel = Kernel()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e822405",
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel.add_service(AzureChatCompletion(\n",
    "    service_id=\"chat\",\n",
    "    deployment_name=environ[\"AZURE_OPENAI_MODEL\"],\n",
    "    endpoint=environ[\"AZURE_OPENAI_ENDPOINT\"],\n",
    "    api_key=environ[\"AZURE_OPENAI_API_KEY\"] ))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70d27331",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KernelPlugin(name='Weather', description=None, functions={'get_sunrise_sunset': KernelFunctionFromMethod(metadata=KernelFunctionMetadata(name='get_sunrise_sunset', plugin_name='Weather', description='Call to get the sunrise and sunset for a given location.', parameters=[KernelParameterMetadata(name='location', description='The location to get the sunrise and sunset', default_value=None, type_='str', is_required=True, type_object=<class 'str'>, schema_data={'type': 'string', 'description': 'The location to get the sunrise and sunset'}, include_in_function_choices=True)], is_prompt=False, is_asynchronous=True, return_parameter=KernelParameterMetadata(name='return', description='', default_value=None, type_='Any', is_required=True, type_object=None, schema_data={'type': 'object'}, include_in_function_choices=True), additional_properties={}), invocation_duration_histogram=<opentelemetry.metrics._internal.instrument._ProxyHistogram object at 0x000002173B257920>, streaming_duration_histogram=<opentelemetry.metrics._internal.instrument._ProxyHistogram object at 0x000002173B2577A0>, method=<function WeatherPlugin.get_sunrise_sunset at 0x0000021737D1F4C0>, stream_method=None), 'get_weather': KernelFunctionFromMethod(metadata=KernelFunctionMetadata(name='get_weather', plugin_name='Weather', description='Call to get the current weather.', parameters=[KernelParameterMetadata(name='location', description='The location to get the current weather', default_value=None, type_='str', is_required=True, type_object=<class 'str'>, schema_data={'type': 'string', 'description': 'The location to get the current weather'}, include_in_function_choices=True)], is_prompt=False, is_asynchronous=True, return_parameter=KernelParameterMetadata(name='return', description='', default_value=None, type_='Any', is_required=True, type_object=None, schema_data={'type': 'object'}, include_in_function_choices=True), additional_properties={}), invocation_duration_histogram=<opentelemetry.metrics._internal.instrument._ProxyHistogram object at 0x000002173B3CC5F0>, streaming_duration_histogram=<opentelemetry.metrics._internal.instrument._ProxyHistogram object at 0x000002173B3CC680>, method=<function WeatherPlugin.get_weather at 0x00000217362ECC20>, stream_method=None)})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kernel.add_plugin(WeatherPlugin, plugin_name=\"Weather\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95e66d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the agent\n",
    "agent = ChatCompletionAgent(\n",
    "    kernel=kernel, \n",
    "    name=\"WeatherAgent\", \n",
    "    instructions=(\n",
    "        \"You are an intelligent weather assistant. \"\n",
    "        \"Use the available plugins and tools to answer questions about the weather, \"\n",
    "        \"including forecasts, temperatures, and conditions. \"\n",
    "        \"Always include temperatures in both Celsius and Fahrenheit. \"\n",
    "        \"Be concise, friendly, and helpful. If you're unsure about a location, ask for clarification.\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2babf387",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The weather in San Francisco today is foggy with a temperature of 60Â°F (15.6Â°C).\n"
     ]
    }
   ],
   "source": [
    "response = await agent.get_response(messages=\"What's the weather like in sf today?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d503630c",
   "metadata": {},
   "source": [
    "## [Handling Intermediate Messages with a ChatCompletionAgent](https://learn.microsoft.com/en-us/semantic-kernel/frameworks/agent/chat-completion-agent?pivots=programming-language-python#handling-intermediate-messages-with-a-chatcompletionagent)\n",
    "\n",
    "The Semantic Kernel ChatCompletionAgent is designed to invoke an agent that fulfills user queries or questions. During invocation, the agent may execute tools to derive the final answer. To access intermediate messages produced during this process, callers can supply a callback function that handles instances of FunctionCallContent or FunctionResultContent.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f39566ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# AuthorRole.USER: 'Hello'\n",
      "# WeatherAgent: Hi there! How can I assist you with the weather today?\n",
      "# AuthorRole.USER: 'What's the weather like in sf today?'\n",
      "# WeatherAgent: In San Francisco today, the temperature is 60Â°F (15.6Â°C) with foggy conditions. Anything else you'd like to know?\n",
      "# AuthorRole.USER: 'What time is the sunrise?'\n",
      "# WeatherAgent: The sunrise in San Francisco is at 6:05 AM, and the sunset is at 8:15 PM. Let me know if you'd like any additional details!\n",
      "# AuthorRole.USER: 'Thank you'\n",
      "# WeatherAgent: You're welcome! Have a great day! ðŸ˜Š\n",
      "\n",
      "Intermediate Steps:\n",
      "Function Call:> Weather-get_weather with arguments: {\"location\":\"San Francisco\"}\n",
      "Function Result:> It's 60 degrees and foggy. for function: Weather-get_weather\n",
      "Function Call:> Weather-get_sunrise_sunset with arguments: {\"location\":\"San Francisco\"}\n",
      "Function Result:> Sunrise: 6:05 A.M, Sunset: 8:15 P.M for function: Weather-get_sunrise_sunset\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from semantic_kernel.contents import AuthorRole, ChatMessageContent, FunctionCallContent, FunctionResultContent\n",
    "\n",
    "\n",
    "# Define a list to hold callback message content\n",
    "intermediate_steps: list[ChatMessageContent] = []\n",
    "\n",
    "# Define an async method to handle the `on_intermediate_message` callback\n",
    "async def handle_intermediate_steps(message: ChatMessageContent) -> None:\n",
    "    intermediate_steps.append(message)\n",
    "\n",
    "\n",
    "\n",
    "user_inputs = [\n",
    "    \"Hello\", \n",
    "    \"What's the weather like in sf today?\", \n",
    "    \"What time is the sunrise?\",\n",
    "    \"Thank you\",\n",
    "]\n",
    "\n",
    "thread = None\n",
    "\n",
    "# Generate the agent response(s)\n",
    "for user_input in user_inputs:\n",
    "    print(f\"# {AuthorRole.USER}: '{user_input}'\")\n",
    "    async for response in agent.invoke(\n",
    "        messages=user_input,\n",
    "        thread=thread,\n",
    "        on_intermediate_message=handle_intermediate_steps,\n",
    "    ):\n",
    "        thread = response.thread\n",
    "        print(f\"# {response.name}: {response.content}\")\n",
    "\n",
    "# Delete the thread when it is no longer needed\n",
    "await thread.delete() if thread else None\n",
    "\n",
    "# Print the intermediate steps\n",
    "print(\"\\nIntermediate Steps:\")\n",
    "for msg in intermediate_steps:\n",
    "    if any(isinstance(item, FunctionResultContent) for item in msg.items):\n",
    "        for fr in msg.items:\n",
    "            if isinstance(fr, FunctionResultContent):\n",
    "                print(f\"Function Result:> {fr.result} for function: {fr.name}\")\n",
    "    elif any(isinstance(item, FunctionCallContent) for item in msg.items):\n",
    "        for fcc in msg.items:\n",
    "            if isinstance(fcc, FunctionCallContent):\n",
    "                print(f\"Function Call:> {fcc.name} with arguments: {fcc.arguments}\")\n",
    "    else:\n",
    "        print(f\"{msg.role}: {msg.content}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
